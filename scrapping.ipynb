{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec404383",
   "metadata": {},
   "source": [
    "# Projet NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a21d55",
   "metadata": {},
   "source": [
    "## Importation des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979272d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d9f66",
   "metadata": {},
   "source": [
    "## Importation des données sur le thème \"accidents de la circulation\" avec l'API de judilibre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61bd4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouverture des fichiers json\n",
    "with open('response_1676987895100.json', 'r') as f1, open('response_1676987729923.json', 'r') as f2 , open('response_1676987924573.json', 'r') as f3, open('response_1676988096056.json', 'r') as f4, open('response_1676988145659.json', 'r') as f5, open('response_1676988236341.json','r') as f6, open('response_1676988349300.json', 'r') as f7, open('response_1676988425273.json', 'r') as f8, open('response_1676988537667.json', 'r') as f9, open('response_1676988615374.json', 'r') as f10:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "    data3 = json.load(f3)\n",
    "    data4 = json.load(f4)\n",
    "    data5 = json.load(f5)\n",
    "    data6 = json.load(f6)\n",
    "    data7 = json.load(f7)\n",
    "    data8 = json.load(f8)\n",
    "    data9 = json.load(f9)\n",
    "    data10 = json.load(f10)\n",
    "    \n",
    "# Transformation des fichiers json en dataframes    \n",
    "df1 = pd.DataFrame(data1['results'])\n",
    "df2 = pd.DataFrame(data2['results'])\n",
    "df3 = pd.DataFrame(data3['results'])\n",
    "df4 = pd.DataFrame(data4['results'])\n",
    "df5 = pd.DataFrame(data5['results'])\n",
    "df6 = pd.DataFrame(data6['results'])\n",
    "df7 = pd.DataFrame(data7['results'])\n",
    "df8 = pd.DataFrame(data8['results'])\n",
    "df9 = pd.DataFrame(data9['results'])\n",
    "df10 = pd.DataFrame(data10['results'])\n",
    "\n",
    "\n",
    "# Concaténation des données en 1 seul dataframe \n",
    "df = pd.concat([df1,df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "df.head()\n",
    "df.columns\n",
    "\n",
    "# Choix des colonnes à conserver \n",
    "df.drop([\"source\", \"jurisdiction\", \"chamber\", \"number\", \"numbers\", \"ecli\", \"publication\", \"update_date\", \"type\", \"nac\", \"portalis\", \"files\", \"contested\", \"forward\", \"timeline\", \"partial\", \"visa\", \"rapprochements\", \"legacy\", \"formation\", \"decision_datetime\", \"update_datetime\", \"zones\", \"solution_alt\"], axis = 1)\n",
    "df.to_csv(\"data_nlp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004f761",
   "metadata": {},
   "source": [
    "## Nettoyage des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NETTOYAGE REGEX\n",
    "def nettoyage_regex(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Enlève les urls\n",
    "    text = re.sub(r'@\\S+', '', text)  # Enlève les nom d'utilisateurs\n",
    "    text = re.sub(r'#\\S+', '', text)  # Enlève les hashtags\n",
    "    \n",
    "    return text\n",
    "\n",
    "### TOKENISATION\n",
    "def tokenisation(text):\n",
    "    tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "    tweet_tokens = [tokenizer.tokenize(text)]\n",
    "    return tweet_tokens[0]\n",
    "\n",
    "### STOP WORDS\n",
    "stop_words = stopwords.words('french')\n",
    "stop_words.append(\"chatgpt\")\n",
    "stop_words.append(\"chat\")\n",
    "stop_words.append(\"gpt\")\n",
    "stop_words.append(\"avoir\")\n",
    "stop_words.append(\"être\")\n",
    "stop_words = set(stop_words)\n",
    "def stop_words_function(text):\n",
    "    return [w.lower() for w in text if not w.lower() in stop_words]\n",
    "\n",
    "## SUPPRESSION PONCTUATION\n",
    "def suppr_ponct(text):\n",
    "    return [token for token in text if token.isalnum()]\n",
    "\n",
    "### LEMMATISATION\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "def lemmatisation(text):\n",
    "    lem = []\n",
    "    for i in range(len(text)):\n",
    "        doc = nlp(text[i])\n",
    "        for token in doc:\n",
    "            lem.append(token.lemma_)\n",
    "    return lem\n",
    "\n",
    "def nettoyage(text):\n",
    "    text = nettoyage_regex(text)\n",
    "    text = tokenisation(text)\n",
    "    text = suppr_ponct(text)\n",
    "    text = lemmatisation(text)\n",
    "    text = stop_words_function(text)\n",
    "    text = [elem.replace('ia', 'intelligence artificielle') for elem in text]\n",
    "    return text\n",
    "\n",
    "def nettoyage_phrase(text):\n",
    "    text = nettoyage_regex(text)\n",
    "    text = tokenisation(text)\n",
    "    return text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
